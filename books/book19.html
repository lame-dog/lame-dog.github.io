<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dive into Deep Learning</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #333;
            /* Dark background color */
            color: #f4f4f4;
            /* Light text color for contrast */
        }

        h1,
        h2,
        h3 {
            color: #f4f4f4;
            /* Light text color for headings */
        }

        h1 {
            font-size: 2em;
            margin-bottom: 0.5em;
        }

        h2 {
            font-size: 1.5em;
            margin-top: 1em;
            margin-bottom: 0.5em;
        }

        h3 {
            font-size: 1.2em;
            margin-top: 0.5em;
            margin-bottom: 0.5em;
        }

        p {
            margin-bottom: 1em;
        }

        .authors {
            font-style: italic;
            margin-bottom: 1.5em;
        }

        .book-cover {
            display: block;
            margin: 0 auto 1.5em;
            width: 100%;
            max-width: 300px;
        }

        .back-link {
            display: block;
            margin-top: 2em;
            text-align: center;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <h1>Dive into Deep Learning</h1>
    <p class="authors">Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola</p>
    <p>PyTorch Edition</p>
    <img src="book19.jpg" alt="Book Cover" class="book-cover">
    <h2>Preface</h2>
    <h3>About This Book</h3>
    <h4>One Medium Combining Code, Math, and HTML</h4>
    <h4>Learning by Doing</h4>
    <h4>Content and Structure</h4>
    <h4>Code</h4>
    <h3>Acknowledgments</h3>
    <h3>Summary</h3>
    <h3>Exercises</h3>
    <h2>Installation</h2>
    <h3>Installing Miniconda</h3>
    <h3>Installing the Deep Learning Framework and the d2l Package</h3>
    <h3>Downloading and Running the Code</h3>
    <h2>Notation</h2>
    <h3>Numerical Objects</h3>
    <h3>Set Theory</h3>
    <h3>Functions and Operators</h3>
    <h3>Calculus</h3>
    <h3>Probability and Information Theory</h3>
    <h2>1. Introduction</h2>
    <h3>1.1 A Motivating Example</h3>
    <h3>1.2 Key Components</h3>
    <h4>1.2.1 Data</h4>
    <h4>1.2.2 Models</h4>
    <h4>1.2.3 Objective Functions</h4>
    <h4>1.2.4 Optimization Algorithms</h4>
    <h3>1.3 Kinds of Machine Learning Problems</h3>
    <h4>1.3.1 Supervised Learning</h4>
    <h5>1.3.1.1 Regression</h5>
    <h5>1.3.1.2 Classification</h5>
    <h5>1.3.1.3 Tagging</h5>
    <h5>1.3.1.4 Search</h5>
    <h5>1.3.1.5 Recommender Systems</h5>
    <h5>1.3.1.6 Sequence Learning</h5>
    <h4>1.3.2 Unsupervised and Self-Supervised Learning</h4>
    <h4>1.3.3 Interacting with an Environment</h4>
    <h4>1.3.4 Reinforcement Learning</h4>
    <h3>1.4 Roots</h3>
    <h3>1.5 The Road to Deep Learning</h3>
    <h3>1.6 Success Stories</h3>
    <h3>1.7 The Essence of Deep Learning</h3>
    <h3>1.8 Summary</h3>
    <h3>1.9 Exercises</h3>
    <h2>2. Preliminaries</h2>
    <h3>2.1 Data Manipulation</h3>
    <h4>2.1.1 Getting Started</h4>
    <h4>2.1.2 Indexing and Slicing</h4>
    <h4>2.1.3 Operations</h4>
    <h4>2.1.4 Broadcasting</h4>
    <h4>2.1.5 Saving Memory</h4>
    <h4>2.1.6 Conversion to Other Python Objects</h4>
    <h4>2.1.7 Summary</h4>
    <h4>2.1.8 Exercises</h4>
    <h3>2.2 Data Preprocessing</h3>
    <h4>2.2.1 Reading the Dataset</h4>
    <h4>2.2.2 Data Preparation</h4>
    <h4>2.2.3 Conversion to the Tensor Format</h4>
    <h4>2.2.4 Discussion</h4>
    <h4>2.2.5 Exercises</h4>
    <h3>2.3 Linear Algebra</h3>
    <h4>2.3.1 Scalars</h4>
    <h4>2.3.2 Vectors</h4>
    <h4>2.3.3 Matrices</h4>
    <h4>2.3.4 Tensors</h4>
    <h4>2.3.5 Basic Properties of Tensor Arithmetic</h4>
    <h4>2.3.6 Reduction</h4>
    <h4>2.3.7 Non-Reduction Sum</h4>
    <h4>2.3.8 Dot Products</h4>
    <h4>2.3.9 Matrix-Vector Products</h4>
    <h4>2.3.10 Matrix-Matrix Multiplication</h4>
    <h4>2.3.11 Norms</h4>
    <h3>2.4. Calculus</h3>
    <h4>2.4.1 Derivatives and Differentiation</h4>
    <h4>2.4.2 Visualization Utilities</h4>
    <h4>2.4.3 Partial Derivatives and Gradients</h4>
    <h4>2.4.4 Chain Rule</h4>
    <h4>2.4.5 Discussion</h4>
    <h4>2.4.6 Exercises</h4>
    <h3>2.5 Automatic Differentiation</h3>
    <h4>2.5.1 A Simple Function</h4>
    <h4>2.5.2 Backward for Non-Scalar Variables</h4>
    <h4>2.5.3 Detaching Computation</h4>
    <h4>2.5.4 Gradients and Python Control Flow</h4>
    <h4>2.5.5 Discussion</h4>
    <h4>2.5.6 Exercises</h4>
    <h3>2.6 Probability and Statistics</h3>
    <h4>2.6.1 A Simple Example: Tossing Coins</h4>
    <h4>2.6.2 A More Formal Treatment</h4>
    <h4>2.6.3 Random Variables</h4>
    <h4>2.6.4 Multiple Random Variables</h4>
    <h4>2.6.5 An Example</h4>
    <h4>2.6.6 Expectations</h4>
    <h4>2.6.7 Discussion</h4>
    <h4>2.6.8 Exercises</h4>
    <h3>2.7 Documentation</h3>
    <h4>2.7.1 Functions and Classes in a Module</h4>
    <h4>2.7.2 Specific Functions and Classes</h4>
    <h2>3. Linear Neural Networks for Regression</h2>
    <h3>3.1 Linear Regression</h3>
    <h4>3.1.1 Basics</h4>
    <h4>3.1.2 Vectorization for Speed</h4>
    <h4>3.1.3 The Normal Distribution and Squared Loss</h4>
    <h4>3.1.4 Linear Regression as a Neural Network</h4>
    <h4>3.1.5 Summary</h4>
    <h4>3.1.6 Exercises</h4>
    <h3>3.2 Object-Oriented Design for Implementation</h3>
    <h4>3.2.1 Utilities</h4>
    <h4>3.2.2 Models</h4>
    <h4>3.2.3 Data</h4>
    <h4>3.2.4 Training</h4>
    <h4>3.2.5 Summary</h4>
    <h4>3.2.6 Exercises</h4>
    <h3>3.3 Synthetic Regression Data</h3>
    <h4>3.3.1 Generating the Dataset</h4>
    <h4>3.3.2 Reading the Dataset</h4>
    <h4>3.3.3 Concise Implementation of the Data Loader</h4>
    <h4>3.3.4 Summary</h4>
    <h4>3.3.5 Exercises</h4>
    <h3>3.4 Linear Regression Implementation from Scratch</h3>
    <h4>3.4.1 Defining the Model</h4>
    <h4>3.4.2 Defining the Loss Function</h4>
    <h4>3.4.3 Defining the Optimization Algorithm</h4>
    <h4>3.4.4 Training</h4>
    <h4>3.4.5 Summary</h4>
    <h4>3.4.6 Exercises</h4>
    <h3>3.5 Concise Implementation of Linear Regression</h3>
    <h4>3.5.1 Defining the Model</h4>
    <h4>3.5.2 Defining the Loss Function</h4>
    <h4>3.5.3 Defining the Optimization Algorithm</h4>
    <h4>3.5.4 Training</h4>
    <h4>3.5.5 Summary</h4>
    <h4>3.5.6 Exercises</h4>
    <h3>3.6 Generalization</h3>
    <h4>3.6.1 Training Error and Generalization Error</h4>
    <h4>3.6.2 Underfitting or Overfitting?</h4>
    <h4>3.6.3 Model Selection</h4>
    <h4>3.6.4 Summary</h4>
    <h4>3.6.5 Exercises</h4>
    <h3>3.7 Weight Decay</h3>
    <h4>3.7.1 Norms and Weight Decay</h4>
    <h4>3.7.2 High-Dimensional Linear Regression</h4>
    <h4>3.7.3 Implementation from Scratch</h4>
    <h4>3.7.4 Concise Implementation</h4>
    <h4>3.7.5 Summary</h4>
    <h4>3.7.6 Exercises</h4>
    <h2>4. Linear Neural Networks for Classification</h2>
    <h3>4.1 Softmax Regression</h3>
    <h4>4.1.1 Classification</h4>
    <h4>4.1.2 Loss Function</h4>
    <h4>4.1.3 Information Theory Basics</h4>
    <h4>4.1.4 Summary and Discussion</h4>
    <h4>4.1.5 Exercises</h4>
    <h3>4.2 The Image Classification Dataset</h3>
    <h4>4.2.1 Loading the Dataset</h4>
    <h4>4.2.2 Reading a Minibatch</h4>
    <h4>4.2.3 Visualization</h4>
    <h4>4.2.4 Summary</h4>
    <h4>4.2.5 Exercises</h4>
    <h3>4.3 The Base Classification Model</h3>
    <h4>4.3.1 The `Classifier` Class</h4>
    <h4>4.3.2 Accuracy</h4>
    <h4>4.3.3 Summary</h4>
    <h4>4.3.4 Exercises</h4>
    <h3>4.4 Softmax Regression Implementation from Scratch</h3>
    <h4>4.4.1 The Softmax</h4>
    <h4>4.4.2 The Model</h4>
    <h4>4.4.3 The Cross-Entropy Loss</h4>
    <h4>4.4.4 Training</h4>
    <h4>4.4.5 Prediction</h4>
    <h4>4.4.6 Summary</h4>
    <h4>4.4.7 Exercises</h4>
    <h3>4.5 Concise Implementation of Softmax Regression</h3>
    <h4>4.5.1 Defining the Model</h4>
    <h4>4.5.2 Softmax Revisited</h4>
    <h4>4.5.3 Training</h4>
    <h4>4.5.4 Summary</h4>
    <h4>4.5.5 Exercises</h4>
    <h3>4.6 Generalization in Classification</h3>
    <h4>4.6.1 The Test Set</h4>
    <h4>4.6.2 Test Set Reuse</h4>
    <h4>4.6.3 Statistical Learning Theory</h4>
    <h4>4.6.4 Summary</h4>
    <h4>4.6.5 Exercises</h4>
    <h3>4.7 Environment and Distribution Shift</h3>
    <h4>4.7.1 Types of Distribution Shift</h4>
    <h4>4.7.2 Examples of Distribution Shift</h4>
    <h4>4.7.3 Correction of Distribution Shift</h4>
    <h4>4.7.4 A Taxonomy of Learning Problems</h4>
    <h4>4.7.5 Fairness, Accountability, and Transparency in Machine Learning</h4>
    <h4>4.7.6 Summary</h4>
    <h4>4.7.7 Exercises</h4>
    <h2>5. Multilayer Perceptrons</h2>
    <h3>5.1 Multilayer Perceptrons</h3>
    <h4>5.1.1 Hidden Layers</h4>
    <h4>5.1.2 Activation Functions</h4>
    <h4>5.1.3 Summary and Discussion</h4>
    <h4>5.1.4 Exercises</h4>
    <h3>5.2 Implementation of Multilayer Perceptrons</h3>
    <h4>5.2.1 Implementation from Scratch</h4>
    <h4>5.2.2 Concise Implementation</h4>
    <h4>5.2.3 Summary</h4>
    <h4>5.2.4 Exercises</h4>
    <h3>5.3 Forward Propagation, Backward Propagation, and Computational Graphs</h3>
    <h4>5.3.1 Forward Propagation</h4>
    <h4>5.3.2 Computational Graph of Forward Propagation</h4>
    <h4>5.3.3 Backpropagation</h4>
    <h4>5.3.4 Training Neural Networks</h4>
    <h4>5.3.5 Summary</h4>
    <h4>5.3.6 Exercises</h4>
    <h3>5.4 Numerical Stability and Initialization</h3>
    <h4>5.4.1 Vanishing and Exploding Gradients</h4>
    <h4>5.4.2 Parameter Initialization</h4>
    <h4>5.4.3 Summary</h4>
    <h4>5.4.4 Exercises</h4>
    <h3>5.5 Generalization in Deep Learning</h3>
    <h4>5.5.1 Revisiting Overfitting and Regularization</h4>
    <h4>5.5.2 Inspiration from Nonparametrics</h4>
    <h4>5.5.3 Early Stopping</h4>
    <h4>5.5.4 Classical Regularization Methods for Deep Networks</h4>
    <h4>5.5.5 Summary</h4>
    <h4>5.5.6 Exercises</h4>
    <h3>5.6 Dropout</h3>
    <h4>5.6.1 Dropout in Practice</h4>
    <h4>5.6.2 Implementation from Scratch</h4>
    <h4>5.6.3 Concise Implementation</h4>
    <h4>5.6.4 Summary</h4>
    <h4>5.6.5 Exercises</h4>
    <h3>5.7 Predicting House Prices on Kaggle</h3>
    <h4>5.7.1 Downloading Data</h4>
    <h4>5.7.2 Kaggle</h4>
    <h4>5.7.3 Accessing and Reading the Dataset</h4>
    <h4>5.7.4 Data Preprocessing</h4>
    <h4>5.7.5 Error Measure</h4>
    <h4>5.7.6 $k$-Fold Cross-Validation</h4>
    <h4>5.7.7 Model Selection</h4>
    <h4>5.7.8 Submitting Predictions on Kaggle</h4>
    <h4>5.7.9 Summary and Discussion</h4>
    <h4>5.7.10 Exercises</h4>
    <h2>6. Builders' Guide</h2>
    <h3>6.1 Layers and Modules</h3>
    <h4>6.1.1 A Custom Module</h4>
    <h4>6.1.2 The Sequential Module</h4>
    <h4>6.1.3 Executing Code in the Forward Propagation Method</h4>
    <h4>6.1.4 Summary</h4>
    <h4>6.1.5 Exercises</h4>
    <h3>6.2 Parameter Management</h3>
    <h4>6.2.1 Parameter Access</h4>
    <h4>6.2.2 Tied Parameters</h4>
    <h4>6.2.3 Summary</h4>
    <h4>6.2.4 Exercises</h4>
    <h3>6.3 Parameter Initialization</h3>
    <h4>6.3.1 Built-in Initialization</h4>
    <h4>6.3.2 Summary</h4>
    <h4>6.3.3 Exercises</h4>
    <h3>6.4 Lazy Initialization</h3>
    <h4>6.4.1 Summary</h4>
    <h4>6.4.2 Exercises</h4>
    <h3>6.5 Custom Layers</h3>
    <h4>6.5.1 Layers without Parameters</h4>
    <h4>6.5.2 Layers with Parameters</h4>
    <h4>6.5.3 Summary</h4>
    <h4>6.5.4 Exercises</h4>
    <h3>6.6 File I/O</h3>
    <h4>6.6.1 Loading and Saving Tensors</h4>
    <h4>6.6.2 Loading and Saving Model Parameters</h4>
    <h4>6.6.3 Summary</h4>
    <h4>6.6.4 Exercises</h4>
    <h3>6.7 GPUs</h3>
    <h4>6.7.1 Computing Devices</h4>
    <h4>6.7.2 Tensors and GPUs</h4>
    <h4>6.7.3 Neural Networks and GPUs</h4>
    <h4>6.7.4 Summary</h4>
    <h4>6.7.5 Exercises</h4>
    <h2>7. Convolutional Neural Networks</h2>
    <h4>11.9.2 Encoder-Decoder</h4>
    <h4>11.9.3 Decoder-Only</h4>
    <h4>11.9.4 Scalability</h4>
    <h4>11.9.5 Large Language Models</h4>
    <h4>11.9.6 Summary and Discussion</h4>
    <h4>11.9.7 Exercises</h4>
    <h2>12. Optimization Algorithms</h2>
    <h3>12.1 Optimization and Deep Learning</h3>
    <h4>12.1.1 Goal of Optimization</h4>
    <h4>12.1.2 Optimization Challenges in Deep Learning</h4>
    <h4>12.1.3 Summary</h4>
    <h4>12.1.4 Exercises</h4>
    <h3>12.2 Convexity</h3>
    <h4>12.2.1 Definitions</h4>
    <h4>12.2.2 Properties</h4>
    <h4>12.2.3 Constraints</h4>
    <h4>12.2.4 Summary</h4>
    <h4>12.2.5 Exercises</h4>
    <h3>12.3 Gradient Descent</h3>
    <h4>12.3.1 One-Dimensional Gradient Descent</h4>
    <h4>12.3.2 Multivariate Gradient Descent</h4>
    <h4>12.3.3 Adaptive Methods</h4>
    <h4>12.3.4 Summary</h4>
    <h4>12.3.5 Exercises</h4>
    <h3>12.4 Stochastic Gradient Descent</h3>
    <h4>12.4.1 Stochastic Gradient Updates</h4>
    <h4>12.4.2 Dynamic Learning Rate</h4>
    <h4>12.4.3 Convergence Analysis for Convex Objectives</h4>
    <h4>12.4.4 Stochastic Gradients and Finite Samples</h4>
    <h4>12.4.5 Summary</h4>
    <h4>12.4.6 Exercises</h4>
    <h3>12.5 Minibatch Stochastic Gradient Descent</h3>
    <h4>12.5.1 Vectorization and Caches</h4>
    <h4>12.5.2 Minibatches</h4>
    <h4>12.5.3 Reading the Dataset</h4>
    <h4>12.5.4 Implementation from Scratch</h4>
    <h4>12.5.5 Concise Implementation</h4>
    <h4>12.5.6 Summary</h4>
    <h4>12.5.7 Exercises</h4>
    <h3>12.6 Momentum</h3>
    <h4>12.6.1 Basics</h4>
    <h4>12.6.2 Practical Experiments</h4>
    <h4>12.6.3 Theoretical Analysis</h4>
    <h4>12.6.4 Summary</h4>
    <h4>12.6.5 Exercises</h4>
    <h3>12.7 Adagrad</h3>
    <h4>12.7.1 Sparse Features and Learning Rates</h4>
    <h4>12.7.2 Preconditioning</h4>
    <h4>12.7.3 The Algorithm</h4>
    <h4>12.7.4 Implementation from Scratch</h4>
    <h4>12.7.5 Concise Implementation</h4>
    <h4>12.7.6 Summary</h4>
    <h4>12.7.7 Exercises</h4>
    <h3>12.8 RMSProp</h3>
    <h4>12.8.1 The Algorithm</h4>
    <h4>12.8.2 Implementation from Scratch</h4>
    <h4>12.8.3 Concise Implementation</h4>
    <h4>12.8.4 Summary</h4>
    <h4>12.8.5 Exercises</h4>
    <h3>12.9 Adadelta</h3>
    <h4>12.9.1 The Algorithm</h4>
    <h4>12.9.2 Implementation</h4>
    <h4>12.9.3 Summary</h4>
    <h4>12.9.4 Exercises</h4>
    <h3>12.10 Adam</h3>
    <h4>12.10.1 The Algorithm</h4>
    <h4>12.10.2 Implementation</h4>
    <h4>12.10.3 Yogi</h4>
    <h4>12.10.4 Summary</h4>
    <h4>12.10.5 Exercises</h4>
    <h3>12.11 Learning Rate Scheduling</h3>
    <h4>12.11.1 Toy Problem</h4>
    <h4>12.11.2 Schedulers</h4>
    <h4>12.11.3 Policies</h4>
    <h4>12.11.4 Summary</h4>
    <h4>12.11.5 Exercises</h4>
    <h2>13. Computational Performance</h2>
    <h3>13.1 Compilers and Interpreters</h3>
    <h4>13.1.1 Symbolic Programming</h4>
    <h4>13.1.2 Hybrid Programming</h4>
    <h4>13.1.3 Hybridizing the `Sequential` Class</h4>
    <h4>13.1.4 Summary</h4>
    <h4>13.1.5 Exercises</h4>
    <h3>13.2 Asynchronous Computation</h3>
    <h4>13.2.1 Asynchrony via Backend</h4>
    <h4>13.2.2 Barriers and Blockers</h4>
    <h4>13.2.3 Improving Computation</h4>
    <h4>13.2.4 Summary</h4>
    <h4>13.2.5 Exercises</h4>
    <h3>13.3 Automatic Parallelism</h3>
    <h4>13.3.1 Parallel Computation on GPUs</h4>
    <h4>13.3.2 Parallel Computation and Communication</h4>
    <h4>13.3.4 Exercises</h4>
    <h3>13.4 Hardware</h3>
    <h4>13.4.1 Computers</h4>
    <h4>13.4.2 Memory</h4>
    <h4>13.4.3 Storage</h4>
    <h4>13.4.4 CPUs</h4>
    <h4>13.4.5 GPUs and other Accelerators</h4>
    <h4>13.4.6 Networks and Buses</h4>
    <h4>13.4.7 More Latency Numbers</h4>
    <h4>13.4.8 Summary</h4>
    <h4>13.4.9 Exercises</h4>
    <h3>13.5 Training on Multiple GPUs</h3>
    <h4>13.5.1 Splitting the Problem</h4>
    <h4>13.5.2 Data Parallelism</h4>
    <h4>13.5.3 A Toy Network</h4>
    <h4>13.5.4 Data Synchronization</h4>
    <h4>13.5.5 Distributing Data</h4>
    <h4>13.5.6 Training</h4>
    <h4>13.5.7 Summary</h4>
    <h4>13.5.8 Exercises</h4>
    <h3>13.6 Concise Implementation for Multiple GPUs</h3>
    <h4>13.6.1 A Toy Network</h4>
    <h4>13.6.2 Network Initialization</h4>
    <h4>13.6.3 Training</h4>
    <h4>13.6.4 Summary</h4>
    <h4>13.6.5 Exercises</h4>
    <h3>13.7 Parameter Servers</h3>
    <h4>13.7.1 Data-Parallel Training</h4>
    <h4>13.7.2 Ring Synchronization</h4>
    <h4>13.7.3 Multi-Machine Training</h4>
    <h4>13.7.4 Key-Value Stores</h4>
    <h4>13.7.5 Summary</h4>
    <h4>13.7.6 Exercises</h4>
    <h2>14. Computer Vision</h2>
    <h3>14.1 Image Augmentation</h3>
    <h4>14.1.1 Common Image Augmentation Methods</h4>
    <h4>14.1.2 Training with Image Augmentation</h4>
    <h4>14.1.3 Summary</h4>
    <h4>14.1.4 Exercises</h4>
    <h3>14.2 Fine-Tuning</h3>
    <h4>14.2.1 Steps</h4>
    <h4>14.2.2 Hot Dog Recognition</h4>
    <h4>14.2.3 Summary</h4>
    <h4>14.2.4 Exercises</h4>
    <h3>14.3 Object Detection and Bounding Boxes</h3>
    <h4>14.3.1 Bounding Boxes</h4>
    <h4>14.3.2 Summary</h4>
    <h4>14.3.3 Exercises</h4>
    <h3>14.4 Anchor Boxes</h3>
    <h4>14.4.1 Generating Multiple Anchor Boxes</h4>
    <h4>14.4.2 Intersection over Union (IoU)</h4>
    <h4>14.4.3 Labeling Anchor Boxes in Training Data</h4>
    <h4>14.4.4 Predicting Bounding Boxes with Non-Maximum Suppression</h4>
    <h4>14.4.5 Summary</h4>
    <h4>14.4.6 Exercises</h4>
    <h3>14.5 Multiscale Object Detection</h3>
    <h4>14.5.1 Multiscale Anchor Boxes</h4>
    <h4>14.5.2 Multiscale Detection</h4>
    <h4>14.5.3 Summary</h4>
    <h4>14.5.4 Exercises</h4>
    <h3>14.6 The Object Detection Dataset</h3>
    <h4>14.6.1 Downloading the Dataset</h4>
    <h4>14.6.2 Reading the Dataset</h4>
    <h4>14.6.3 Demonstration</h4>
    <h4>14.6.4 Summary</h4>
    <h4>14.6.5 Exercises</h4>
    <h3>14.7 Single Shot Multibox Detection</h3>
    <h4>14.7.1 Model</h4>
    <h4>14.7.2 Training</h4>
    <h4>14.7.3 Prediction</h4>
    <h4>14.7.4 Summary</h4>
    <h4>14.7.5 Exercises</h4>
    <h3>14.8 Region-based CNNs (R-CNNs)</h3>
    <h4>14.8.1 R-CNNs</h4>
    <h4>14.8.2 Fast R-CNN</h4>
    <h4>14.8.3 Faster R-CNN</h4>
    <h4>14.8.4 Mask R-CNN</h4>
    <h4>14.8.5 Summary</h4>
    <h4>14.8.6 Exercises</h4>
    <h3>14.9 Semantic Segmentation and the Dataset</h3>
    <h4>14.9.1 Image Segmentation and Instance Segmentation</h4>
    <h4>14.9.2 The Pascal VOC2012 Semantic Segmentation Dataset</h4>
    <h4>14.9.3 Summary</h4>
    <h4>14.9.4 Exercises</h4>
    <h3>14.10 Transposed Convolution</h3>
    <h4>14.10.1 Basic Operation</h4>
    <h4>14.10.2 Padding, Strides, and Multiple Channels</h4>
    <h4>14.10.3 Connection to Matrix Transposition</h4>
    <h4>14.10.4 Summary</h4>
    <h4>14.10.5 Exercises</h4>
    <h3>14.11 Fully Convolutional Networks</h3>
    <h4>14.11.1 The Model</h4>
    <h4>14.11.2 Initializing Transposed Convolutional Layers</h4>
    <h4>14.11.3 Reading the Dataset</h4>
    <h4>14.11.4 Training</h4>
    <h4>14.11.5 Prediction</h4>
    <h4>14.11.6 Summary</h4>
    <h4>14.11.7 Exercises</h4>
    <h3>14.12 Neural Style Transfer</h3>
    <h4>14.12.1 Method</h4>
    <h4>14.12.2 Reading the Content and Style Images</h4>
    <h4>14.12.3 Preprocessing and Postprocessing</h4>
    <h4>14.12.4 Extracting Features</h4>
    <h4>14.12.5 Defining the Loss Function</h4>
    <h4>14.12.6 Initializing the Synthesized Image</h4>
    <h4>14.12.7 Training</h4>
    <h4>14.12.8 Summary</h4>
    <h4>14.12.9 Exercises</h4>
    <h3>14.13 Image Classification (CIFAR-10) on Kaggle</h3>
    <h4>14.13.1 Obtaining and Organizing the Dataset</h4>
    <h4>14.13.2 Image Augmentation</h4>
    <h4>14.13.3 Reading the Dataset</h4>
    <h4>14.13.4 Defining the Model</h4>
    <h4>14.13.5 Defining the Training Function</h4>
    <h4>14.13.6 Training and Validating the Model</h4>
    <h4>14.13.7 Classifying the Testing Set and Submitting Results on Kaggle</h4>
    <h4>14.13.8 Summary</h4>
    <h4>14.13.9 Exercises</h4>
    <h3>14.14 Dog Breed Identification (ImageNet Dogs) on Kaggle</h3>
    <h4>14.14.1 Obtaining and Organizing the Dataset</h4>
    <h4>14.14.2 Image Augmentation</h4>
    <h4>14.14.3 Reading the Dataset</h4>
    <h4>14.14.4 Fine-Tuning a Pretrained Model</h4>
    <h4>14.14.5 Defining the Training Function</h4>
    <h4>14.14.6 Training and Validating the Model</h4>
    <h4>14.14.7 Classifying the Testing Set and Submitting Results on Kaggle</h4>
    <h4>14.14.8 Summary</h4>
    <h4>14.14.9 Exercises</h4>
    <h2>15. Natural Language Processing: Pretraining</h2>
    <h3>15.1 Word Embedding (word2vec)</h3>
    <h4>15.1.1 One-Hot Vectors Are a Bad Choice</h4>
    <h4>15.1.2 Self-Supervised word2vec</h4>
    <h4>15.1.3 The Skip-Gram Model</h4>
    <h4>15.1.4 The Continuous Bag of Words (CBOW) Model</h4>
    <h4>15.1.5 Summary</h4>
    <h4>15.1.6 Exercises</h4>
    <h3>15.2 Approximate Training</h3>
    <h4>15.2.1 Negative Sampling</h4>
    <h4>15.2.2 Hierarchical Softmax</h4>
    <h4>15.2.3 Summary</h4>
    <h4>15.2.4 Exercises</h4>
    <h3>15.3 The Dataset for Pretraining Word Embeddings</h3>
    <h4>15.3.1 Reading the Dataset</h4>
    <h4>15.3.2 Subsampling</h4>
    <h4>15.3.3 Extracting Center Words and Context Words</h4>
    <h4>15.3.4 Negative Sampling</h4>
    <h4>15.3.5 Loading Training Examples in Minibatches</h4>
    <h4>15.3.6 Putting It All Together</h4>
    <h4>15.3.7 Summary</h4>
    <h4>15.3.8 Exercises</h4>
    <h3>15.4 Pretraining word2vec</h3>
    <h4>15.4.1 The Skip-Gram Model</h4>
    <h4>15.4.2 Training</h4>
    <h4>15.4.3 Applying Word Embeddings</h4>
    <h4>15.4.4 Summary</h4>
    <h4>15.4.5 Exercises</h4>
    <h3>15.5 Word Embedding with Global Vectors (GloVe)</h3>
    <h4>15.5.1 Skip-Gram with Global Corpus Statistics</h4>
    <h4>15.5.2 The GloVe Model</h4>
    <h4>15.5.3 Interpreting GloVe from the Ratio of Co-occurrence Probabilities</h4>
    <h4>15.5.4 Summary</h4>
    <h4>15.5.5 Exercises</h4>
    <h3>15.6 Subword Embedding</h3>
    <h4>15.6.1 The fastText Model</h4>
    <h4>15.6.2 Byte Pair Encoding</h4>
    <h4>15.6.3 Summary</h4>
    <h4>15.6.4 Exercises</h4>
    <h3>15.7 Word Similarity and Analogy</h3>
    <h4>15.7.1 Loading Pretrained Word Vectors</h4>
    <h4>15.7.2 Applying Pretrained Word Vectors</h4>
    <h4>15.7.3 Summary</h4>
    <h4>15.7.4 Exercises</h4>
    <h3>15.8 Bidirectional Encoder Representations from Transformers (BERT)</h3>
    <h4>15.8.1 From Context-Independent to Context-Sensitive</h4>
    <h4>15.8.2 From Task-Specific to Task-Agnostic</h4>
    <h4>15.8.3 BERT: Combining the Best of Both Worlds</h4>
    <h4>15.8.4 Input Representation</h4>
    <h4>15.8.5 Pretraining Tasks</h4>
    <h4>15.8.6 Putting It All Together</h4>
    <h4>15.8.7 Summary</h4>
    <h4>15.8.8 Exercises</h4>
    <h3>15.9 The Dataset for Pretraining BERT</h3>
    <h4>15.9.1 Defining Helper Functions for Pretraining Tasks</h4>
    <h4>15.9.2 Transforming Text into the Pretraining Dataset</h4>
    <h4>15.9.3 Summary</h4>
    <h4>15.9.4 Exercises</h4>
    <h3>15.10 Pretraining BERT</h3>
    <h4>15.10.1 Pretraining BERT</h4>
    <h4>15.10.2 Representing Text with BERT</h4>
    <h4>15.10.3 Summary</h4>
    <h4>15.10.4 Exercises</h4>
    <h2>16. Natural Language Processing: Applications</h2>
    <h3>16.1 Sentiment Analysis and the Dataset</h3>
    <h4>16.1.1 Reading the Dataset</h4>
    <h4>16.1.2 Preprocessing the Dataset</h4>
    <h4>16.1.3 Creating Data Iterators</h4>
    <h4>16.1.4 Putting It All Together</h4>
    <h4>16.1.5 Summary</h4>
    <h4>16.1.6 Exercises</h4>
    <h3>16.2 Sentiment Analysis: Using Recurrent Neural Networks</h3>
    <h4>16.2.1 Representing Single Text with RNNs</h4>
    <h4>16.2.2 Loading Pretrained Word Vectors</h4>
    <h4>16.2.3 Training and Evaluating the Model</h4>
    <h4>16.2.4 Summary</h4>
    <h4>16.2.5 Exercises</h4>
    <h3>16.3 Sentiment Analysis: Using Convolutional Neural Networks</h3>
    <h4>16.3.1 One-Dimensional Convolutions</h4>
    <h4>16.3.2 Max-Over-Time Pooling</h4>
    <h4>16.3.3 The textCNN Model</h4>
    <h4>16.3.4 Summary</h4>
    <h4>16.3.5 Exercises</h4>
    <h3>16.4 Natural Language Inference and the Dataset</h3>
    <h4>16.4.1 Natural Language Inference</h4>
    <h4>16.4.2 The Stanford Natural Language Inference (SNLI) Dataset</h4>
    <h4>16.4.3 Summary</h4>
    <h4>16.4.4 Exercises</h4>
    <h3>16.5 Natural Language Inference: Using Attention</h3>
    <h4>16.5.1 The Model</h4>
    <h4>16.5.2 Training and Evaluating the Model</h4>
    <h4>16.5.3 Summary</h4>
    <h4>16.5.4 Exercises</h4>
    <h3>16.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications</h3>
    <h4>16.6.1 Single Text Classification</h4>
    <h4>16.6.2 Text Pair Classification or Regression</h4>
    <h4>16.6.3 Text Tagging</h4>
    <h4>16.6.4 Question Answering</h4>
    <h4>16.6.5 Summary</h4>
    <h4>16.6.6 Exercises</h4>
    <h3>16.7 Natural Language Inference: Fine-Tuning BERT</h3>
    <h4>16.7.1 Loading Pretrained BERT</h4>
    <h4>16.7.2 The Dataset for Fine-Tuning BERT</h4>
    <h4>16.7.3 Fine-Tuning BERT</h4>
    <h4>16.7.4 Summary</h4>
    <h4>16.7.5 Exercises</h4>
    <h2>17. Reinforcement Learning</h2>
    <h3>17.1 Markov Decision Process (MDP)</h3>
    <h4>17.1.1 Definition of an MDP</h4>
    <h4>17.1.2 Return and Discount Factor</h4>
    <h4>17.1.3 Discussion of the Markov Assumption</h4>
    <h4>17.1.4 Summary</h4>
    <h4>17.1.5 Exercises</h4>
    <h3>17.2 Value Iteration</h3>
    <h4>17.2.1 Stochastic Policy</h4>
    <h4>17.2.2 Value Function</h4>
    <h4>17.2.3 Action-Value Function</h4>
    <h4>17.2.4 Optimal Stochastic Policy</h4>
    <h4>17.2.5 Principle of Dynamic Programming</h4>
    <h4>17.2.6 Value Iteration</h4>
    <h4>17.2.7 Policy Evaluation</h4>
    <h4>17.2.8 Implementation of Value Iteration</h4>
    <h4>17.2.9 Summary</h4>
    <h4>17.2.10 Exercises</h4>
    <h3>17.3 Q-Learning</h3>
    <h4>17.3.1 The Q-Learning Algorithm</h4>
    <h4>17.3.2 An Optimization Problem Underlying Q-Learning</h4>
    <h4>17.3.3 Exploration in Q-Learning</h4>
    <h4>17.3.4 The “Self-correcting” Property of Q-Learning</h4>
    <h4>17.3.5 Implementation of Q-Learning</h4>
    <h4>17.3.6 Summary</h4>
    <h4>17.3.7 Exercises</h4>
    <h2>18. Gaussian Processes</h2>
    <h3>18.1 Introduction to Gaussian Processes</h3>
    <h4>18.1.1 Summary</h4>
    <h4>18.1.2 Exercises</h4>
    <h3>18.2 Gaussian Process Priors</h3>
    <h4>18.2.1 Definition</h4>
    <h4>18.2.2 A Simple Gaussian Process</h4>
    <h4>18.2.3 From Weight Space to Function Space</h4>
    <h4>18.2.4 The Radial Basis Function (RBF) Kernel</h4>
    <h4>18.2.5 The Neural Network Kernel</h4>
    <h4>18.2.6 Summary</h4>
    <h4>18.2.7 Exercises</h4>
    <h3>18.3 Gaussian Process Inference</h3>
    <h4>18.3.1 Posterior Inference for Regression</h4>
    <h4>18.3.2 Equations for Making Predictions and Learning Kernel Hyperparameters in GP Regression</h4>
    <h4>18.3.3 Interpreting Equations for Learning and Predictions</h4>
    <h4>18.3.4 Worked Example from Scratch</h4>
    <h4>18.3.5 Making Life Easy with GPyTorch</h4>
    <h4>18.3.6 Summary</h4>
    <h4>18.3.7 Exercises</h4>
    <h4>18.3.7 Exercises</h4>
    <h2>19. Hyperparameter Optimization</h2>
    <h3>19.1 What Is Hyperparameter Optimization?</h3>
    <h4>19.1.1 The Optimization Problem</h4>
    <h4>19.1.2 Random Search</h4>
    <h4>19.1.3 Summary</h4>
    <h4>19.1.4 Exercises</h4>
    <h3>19.2 Hyperparameter Optimization API</h3>
    <h4>19.2.1 Searcher</h4>
    <h4>19.2.2 Scheduler</h4>
    <h4>19.2.3 Tuner</h4>
    <h4>19.2.4 Bookkeeping the Performance of HPO Algorithms</h4>
    <h4>19.2.5 Example: Optimizing the Hyperparameters of a Convolutional Neural Network</h4>
    <h4>19.2.6 Comparing HPO Algorithms</h4>
    <h4>19.2.7 Summary</h4>
    <h4>19.2.8 Exercises</h4>
    <h3>19.3 Asynchronous Random Search</h3>
    <h4>19.3.1 Objective Function</h4>
    <h4>19.3.2 Asynchronous Scheduler</h4>
    <h4>19.3.3 Visualize the Asynchronous Optimization Process</h4>
    <h4>19.3.4 Summary</h4>
    <h4>19.3.5 Exercises</h4>
    <h3>19.4 Multi-Fidelity Hyperparameter Optimization</h3>
    <h4>19.4.1 Successive Halving</h4>
    <h4>19.4.2 Summary</h4>
    <h3>19.5 Asynchronous Successive Halving</h3>
    <h4>19.5.1 Objective Function</h4>
    <h4>19.5.2 Asynchronous Scheduler</h4>
    <h4>19.5.3 Visualize the Optimization Process</h4>
    <h4>19.5.4 Summary</h4>
    <h2>20. Generative Adversarial Networks</h2>
    <h3>20.1 Generative Adversarial Networks</h3>
    <h4>20.1.1 Generate Some "Real" Data</h4>
    <h4>20.1.2 Generator</h4>
    <h4>20.1.3 Discriminator</h4>
    <h4>20.1.4 Training</h4>
    <h4>20.1.5 Summary</h4>
    <h4>20.1.6 Exercises</h4>
    <h3>20.2 Deep Convolutional Generative Adversarial Networks</h3>
    <h4>20.2.1 The Pokemon Dataset</h4>
    <h4>20.2.2 The Generator</h4>
    <h4>20.2.3 Discriminator</h4>
    <h4>20.2.4 Training</h4>
    <h4>20.2.5 Summary</h4>
    <h4>20.2.6 Exercises</h4>
    <h2>21. Recommender Systems</h2>
    <h3>21.1 Overview of Recommender Systems</h3>
    <h4>21.1.1 Collaborative Filtering</h4>
    <h4>21.1.2 Explicit Feedback and Implicit Feedback</h4>
    <h4>21.1.3 Recommendation Tasks</h4>
    <h4>21.1.4 Summary</h4>
    <h4>21.1.5 Exercises</h4>
    <h2>A. Mathematics for Deep Learning</h2>
    <h3>A.1 Geometry and Linear Algebraic Operations</h3>
    <h4>A.1.1 Geometry of Vectors</h4>
    <h4>A.1.2 Dot Products and Angles</h4>
    <h4>A.1.3 Hyperplanes</h4>
    <h4>A.1.4 Geometry of Linear Transformations</h4>
    <h4>A.1.5 Linear Dependence</h4>
    <h4>A.1.6 Rank</h4>
    <h4>A.1.7 Invertibility</h4>
    <h4>A.1.8 Determinant</h4>
    <h4>A.1.9 Tensors and Common Linear Algebra Operations</h4>
    <h4>A.1.10 Summary</h4>
    <h4>A.1.11 Exercises</h4>
    <h3>A.2 Eigendecompositions</h3>
    <h4>A.2.1 Finding Eigenvalues</h4>
    <h4>A.2.2 Decomposing Matrices</h4>
    <h4>A.2.3 Operations on Eigendecompositions</h4>
    <h4>A.2.4 Eigendecompositions of Symmetric Matrices</h4>
    <h4>A.2.5 Gershgorin Circle Theorem</h4>
    <h4>A.2.6 A Useful Application: The Growth of Iterated Maps</h4>
    <h4>A.2.7 Discussion</h4>
    <h4>A.2.8 Summary</h4>
    <h4>A.2.9 Exercises</h4>
    <h3>A.3 Single Variable Calculus</h3>
    <h4>A.3.1 Differential Calculus</h4>
    <h4>A.3.2 Rules of Calculus</h4>
    <h4>A.3.3 Summary</h4>
    <h4>A.3.4 Exercises</h4>
    <h3>A.4 Multivariable Calculus</h3>
    <h4>A.4.1 Higher-Dimensional Differentiation</h4>
    <h4>A.4.2 Geometry of Gradients and Gradient Descent</h4>
    <h4>A.4.3 A Note on Mathematical Optimization</h4>
    <h4>A.4.4 Multivariate Chain Rule</h4>
    <h4>A.4.5 The Backpropagation Algorithm</h4>
    <h4>A.4.6 Hessians</h4>
    <h4>A.4.7 A Little Matrix Calculus</h4>
    <h4>A.4.8 Summary</h4>
    <h4>A.4.9 Exercises</h4>
    <h3>A.5 Integral Calculus</h3>
    <h4>A.5.1 Geometric Interpretation</h4>
    <h4>A.5.2 The Fundamental Theorem of Calculus</h4>
    <h4>A.5.3 Change of Variables</h4>
    <h4>A.5.4 A Comment on Sign Conventions</h4>
    <h4>A.5.5 Multiple Integrals</h4>
    <h4>A.5.6 Change of Variables in Multiple Integrals</h4>
    <h4>A.5.7 Summary</h4>
    <h4>A.5.8 Exercises</h4>
    <h3>A.6 Random Variables</h3>
    <h4>A.6.1 Continuous Random Variables</h4>
    <h4>A.6.2 Summary</h4>
    <h4>A.6.3 Exercises</h4>
    <h3>A.7 Maximum Likelihood</h3>
    <h4>A.7.1 The Maximum Likelihood Principle</h4>
    <h4>A.7.2 Numerical Optimization and the Negative Log-Likelihood</h4>
    <h4>A.7.3 Maximum Likelihood for Continuous Variables</h4>
    <h4>A.7.4 Summary</h4>
    <h4>A.7.5 Exercises</h4>
    <h3>A.8 Distributions</h3>
    <h4>A.8.1 Bernoulli</h4>
    <h4>A.8.2 Discrete Uniform</h4>
    <h4>A.8.3 Continuous Uniform</h4>
    <h4>A.8.4 Binomial</h4>
    <h4>A.8.5 Poisson</h4>
    <h4>A.8.6 Gaussian</h4>
    <h4>A.8.7 Exponential Family</h4>
    <h4>A.8.8 Summary</h4>
    <h4>A.8.9 Exercises</h4>
    <h3>A.9 Naive Bayes</h3>
    <h4>A.9.1 Optical Character Recognition</h4>
    <h4>A.9.2 The Probabilistic Model for Classification</h4>
    <h4>A.9.3 The Naive Bayes Classifier</h4>
    <h4>A.9.4 Training</h4>
    <h4>A.9.5 Summary</h4>
    <h4>A.9.6 Exercises</h4>
    <h3>A.10 Statistics</h3>
    <h4>A.10.1 Evaluating and Comparing Estimators</h4>
    <h4>A.10.2 Conducting Hypothesis Tests</h4>
    <h4>A.10.3 Constructing Confidence Intervals</h4>
    <h4>A.10.4 Summary</h4>
    <h4>A.10.5 Exercises</h4>
    <h3>A.11 Information Theory</h3>
    <h4>A.11.1 Information</h4>
    <h4>A.11.2 Entropy</h4>
    <h4>A.11.3 Mutual Information</h4>
    <h4>A.11.4 Kullback-Leibler Divergence</h4>
    <h4>A.11.5 Cross-Entropy</h4>
    <h4>A.11.6 Summary</h4>
    <h4>A.11.7 Exercises</h4>
    <h2>B. Tools for Deep Learning</h2>
    <h3>B.1 Using Jupyter Notebooks</h3>
    <h4>B.1.1 Editing and Running the Code Locally</h4>
    <h4>B.1.2 Advanced Options</h4>
    <h4>B.1.3 Summary</h4>
    <h4>B.1.4 Exercises</h4>
    <h3>B.2 Using Amazon SageMaker</h3>
    <h4>B.2.1 Signing Up</h4>
    <h4>B.2.2 Creating a SageMaker Instance</h4>
    <h4>B.2.3 Running and Stopping an Instance</h4>
    <h4>B.2.4 Updating Notebooks</h4>
    <h4>B.2.5 Summary</h4>
    <h4>B.2.6 Exercises</h4>
    <h3>B.3 Using AWS EC2 Instances</h3>
    <h4>B.3.1 Creating and Running an EC2 Instance</h4>
    <h4>B.3.2 Installing CUDA</h4>
    <h4>B.3.3 Installing Libraries for Running the Code</h4>
    <h4>B.3.4 Running the Jupyter Notebook remotely</h4>
    <h4>B.3.5 Closing Unused Instances</h4>
    <h4>B.3.6 Summary</h4>
    <h4>B.3.7 Exercises</h4>
    <h3>B.4 Using Google Colab</h3>
    <h4>B.4.1 Summary</h4>
    <h3>B.5 Selecting Servers and GPUs</h3>
    <h4>B.5.1 Selecting Servers</h4>
    <h4>B.5.2 Selecting GPUs</h4>
    <h4>B.5.3 Summary</h4>
    <h3>B.6 Contributing to This Book</h3>
    <h4>B.6.1 Submitting Minor Changes</h4>
    <h4>B.6.2 Proposing Major Changes</h4>
    <h4>B.6.3 Submitting Major Changes</h4>
    <h4>B.6.4 Summary</h4>
    <h4>B.6.5 Exercises</h4>
    <h3>B.7 Utility Functions and Classes</h3>
    <h3>B.8 The d2l API Document</h3>
    <h4>B.8.1 Classes</h4>
</body>